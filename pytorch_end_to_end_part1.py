# -*- coding: utf-8 -*-
"""Pytorch_end_to_end_part1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L6fe1IspfjzjL9HTL_wMT9-FgDqB_rvj

Get data ready--> build a model ->Fitting the model to data ->
Making predictions and evaluating -> saving and loading a model -> putting all together
"""

import torch
from torch import nn #nn contains all pytorch's building bloacks for neural network
import matplotlib.pyplot as plt

torch.__version__

"""here first we create data , simple straight line data with known parameters."""

weight = 0.6
bias   = 0.4

start = 0
end   = 1
step = 0.02

X = torch.arange(start, end, step).unsqueeze(dim = 1)
y = weight * X + bias

X[:10], y[:10]

"""Splitting data into training and testing set."""

mark = int(0.8 *len(X))

X_train, y_train = X[:mark], y[:mark]
X_test, y_test = X[mark:], y[mark:]

len(X_train), len(y_train), len(X_test), len(y_test)

"""Visualizing our data"""

def plot_predictions(train_data = X_train,
                     train_labels = y_train,
                     test_data = X_test,
                     test_label = y_test,
                     predictions = None):

  plt.figure(figsize=(10, 7))

  plt.scatter(train_data, train_labels, c = "b",s = 4, label = "Training data")

  plt.scatter(test_data, test_label, c="g", s=4, label ="Testing data")

  if predictions is not None:
    plt.scatter(test_data, predictions, c="r", s=4, label = "predictions")

  plt.legend(prop={"size": 14});

plot_predictions();

"""Building a model"""

class LinearRegressionModel(nn.Module):
  def __init__(self):
    super().__init__()
    self.weights = nn.Parameter(torch.randn(1, # starting with random weights
                                            dtype = torch.float),
                                requires_grad = True)
    self.bias = nn.Parameter(torch.randn(1,
                                         dtype = torch.float),
                             requires_grad = True)

  def forward(self, x:torch.Tensor) -> torch.Tensor:
    return self.weights*x + self.bias

torch.manual_seed(42)

model_00 = LinearRegressionModel()

list(model_00.parameters())

model_00.state_dict()

with torch.inference_mode():
  y_preds = model_00(X_test)

X_test

print(f"Number of testing samples: {len(X_test)}")
print(f"Number of prediction samples: {len(y_preds)}")
print(f"Predicted values:\n {y_preds}")

plot_predictions(predictions = y_preds) # these predictions are from randomly selected parameters that is why so much difference is observed.

"""Train the model"""

loss_fn = nn.L1Loss() #MAE mean absolute error
optimizer = torch.optim.SGD(params = model_00.parameters(),
                            lr = 0.01) # learning rate

torch.manual_seed(42)
epochs = 110

train_loss_values = []
test_loss_values = []
epoch_count = []

for epoch in range(epochs):
  # putting model in training mode
  model_00.train()

  #forward pass
  y_preds = model_00(X_train)

  #calculate the loss
  loss = loss_fn(y_preds, y_train)

  #zero grad of the optimizer
  optimizer.zero_grad()

  #loss backward
  loss.backward()

  #progress the optimizer
  optimizer.step()


  # testing mode
  #putting model in testing mode

  model_00.eval()
  with torch.inference_mode():
    #forward pass
    test_pred = model_00(X_test)

    #calculate the loss
    test_loss = loss_fn(test_pred, y_test)

    # in between observation
    if epoch % 10 == 0:
      epoch_count.append(epoch)
      train_loss_values.append(loss.detach().numpy())
      test_loss_values.append(test_loss.detach().numpy())
      print(f"Epoch:{epoch} |MAE train loss:{loss} | MAE test loss:{test_loss}")

# plotting the loss curves
plt.plot(epoch_count,train_loss_values, label = "Train loss")
plt.plot(epoch_count, test_loss_values, label = "Test loss")
plt.title("Traing and test loss curves")
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.legend();

print("Model_00 learned the following values for weight and bias")
print(model_00.state_dict())
print("\n And the original values for weight and bias")
print(f"weight: {weight}, bias: {bias}")

"""Now we try the same thing for 200 epoch"""

torch.manual_seed(42)
model_001 = LinearRegressionModel()
loss_fn = nn.L1Loss()
optimizer = torch.optim.SGD(params = model_001.parameters(),
                            lr = 0.01)

epochs = 200

train_loss_values1 = []
test_loss_values1 =[]
epoch_count1 =[]

for epoch in range(epochs):
  #training mode
  model_001.train()
  y_pred = model_001(X_train)
  loss = loss_fn(y_pred, y_train)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()


  #testing
  model_001.eval()
  with torch.inference_mode():
    test_pred = model_001(X_test)
    test_loss = loss_fn(test_pred,y_test.type(torch.float))

    if epoch % 10 == 0:
      epoch_count1.append(epoch)
      train_loss_values1.append(loss.detach().numpy())
      test_loss_values1.append(test_loss.detach().numpy())
      print(f"Epoch:{epoch} |MAE train loss:{loss} | MAE test loss:{test_loss}")

# plotting the loss curves
plt.plot(epoch_count,train_loss_values1, label = "Train loss")
plt.plot(epoch_count, test_loss_values1, label = "Test loss")
plt.title("Traing and test loss curves")
plt.ylabel("Loss")
plt.xlabel("Epochs")
plt.legend();

print(f"model 2 parameters : {model_001.state_dict()} ")
print(f"model 1 parameters : {model_00.state_dict()} ")

"""making predictions with our trained model which is also called performing inference with model.
Three things are needed to be done
1) set the model in evaluation mode.

2)make predictions using inference mode the context manager ( with torch.inference_mode():  )

3)all predictions should be made with objects on the same device.
"""

model_00.eval()

with torch.inference_mode():
  y_preds = model_00(X_test)

y_preds

plot_predictions(predictions = y_preds)

"""Saving and loading a PyTorch model."""

from pathlib import Path

# create model directory
MODEL_PATH = Path("models")
MODEL_PATH.mkdir(parents = True, exist_ok = True)

# create model save path
MODEL_NAME = "lin_reg_pytorch_model_00.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

# save the model state dict
print(f"saving model to :{MODEL_SAVE_PATH}")
torch.save(obj = model_00.state_dict(),
           f= MODEL_SAVE_PATH)

#checking save file path
!ls -l models/lin_reg_pytorch_model_00.pth

"""Loading a saved pytorch model"""

loaded_model_00 = LinearRegressionModel()

loaded_model_00.load_state_dict(torch.load(f = MODEL_SAVE_PATH))

"""Testing the loaded model

"""

# putting the loaded model in eval mode
loaded_model_00.eval()
with torch.inference_mode():
  loaded_model_preds = loaded_model_00(X_test)

y_preds == loaded_model_preds

"""# Putting it all together"""

import torch
from torch import nn
import matplotlib.pyplot as plt

torch.__version__

# setting up device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device} ")

# data
weight = 0.7
bias = 0.3

start = 0
end = 1
step = 0.02

X= torch.arange(start,end, step).unsqueeze(dim = 1)
y = weight*X + bias

X[:10], y[:10]

# data splitting
mark = int(0.8*len(X))
X_train, y_train = X[:mark], y[:mark]
X_test, y_test = X[mark:], y[mark:]

len(X_train), len(y_train), len(X_test), len(y_test)

def plot_predictions2(train_data=X_train,
                     train_labels = y_train,
                     test_data = X_test,
                     test_labels = y_test,
                     predictions = None):

  plt.figure(figsize=(10, 7))

  plt.scatter(train_data, train_labels, c = "b",s = 4, label = "Training data")

  plt.scatter(test_data, test_labels, c="g", s=4, label ="Testing data")

  if predictions is not None:
    plt.scatter(test_data, predictions, c="r", s=4, label = "predictions")

  plt.legend(prop={"size": 14});

plot_predictions2(X_train,y_train, X_test, y_test)

# building a pytorch linear model
class LinearRegressionModelV2(nn.Module):
  def __init__(self):
    super().__init__()
    self.linear_layer = nn.Linear(in_features = 1,
                                  out_features = 1)

  def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.linear_layer(x)


torch.manual_seed(42)
model_v2_00 = LinearRegressionModelV2()
model_v2_00 , model_v2_00.state_dict()

next(model_v2_00.parameters()).device

model_v2_00.to(device)
next(model_v2_00.parameters()).device

#training
loss_fn = nn.L1Loss()
optimizer = torch.optim.SGD(params = model_v2_00.parameters(),
                            lr = 0.01)

torch.manual_seed(42)
epochs = 1000

#since model is on cuda , we also need to put our data on same as model that is cuda
X_train = X_train.to(device)
y_train = y_train.to(device)
X_test = X_test.to(device)
y_test = y_test.to(device)

for epoch in range(epochs):
  model_v2_00.train()
  y_pred = model_v2_00(X_train)
  loss =loss_fn(y_pred, y_train)
  optimizer.zero_grad()
  loss.backward()
  optimizer.step()

  ## testing
  model_v2_00.eval()
  with torch.inference_mode():
    test_pred = model_v2_00(X_test)
    test_loss = loss_fn(test_pred, y_test)

  if epoch%10 == 0 :
    print(f"EPoch:{epoch} | Train loss : {loss} | Test loss: {test_loss}")

from pprint import pprint
print("The model learned the following values for weights and bias")
pprint(model_v2_00.state_dict())
print("\n And the original values for weights and bias")
print(f"weight: {weight}, bias:{bias}")

# evaluating our model
model_v2_00.eval()
with torch.inference_mode():
  y_preds = model_v2_00(X_test)

y_preds

plot_predictions2(predictions = y_preds.cpu())

# saving and loading model
from pathlib import Path

MODEL_PATH = Path("models2")
MODEL_PATH.mkdir(parents=True, exist_ok = True)

MODEL_NAME = "02_lin_reg_pytorch_model.pth"
MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME

print(f"saving model to :{MODEL_SAVE_PATH}")
torch.save(obj = model_v2_00.state_dict(),
           f= MODEL_SAVE_PATH)

#loading it back again
loaded_model_v2 = LinearRegressionModelV2()
loaded_model_v2.load_state_dict(torch.load(MODEL_SAVE_PATH))

loaded_model_v2.to(device)
print(f"loaded model: {loaded_model_v2}")
print(f"Model on device: \n {next(loaded_model_v2.parameters()).device}")

#evaluating our loaded model
loaded_model_v2.eval()
with torch.inference_mode():
  loaded_model_v2_pred = loaded_model_v2(X_test)

y_preds == loaded_model_v2_pred